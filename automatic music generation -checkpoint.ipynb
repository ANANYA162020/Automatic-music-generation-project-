{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## library for understanding music "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from music21 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_midi(file):\n",
    "    \n",
    "    print(\"Loading Music File:\",file)\n",
    "    \n",
    "    notes=[]\n",
    "    notes_to_parse = None\n",
    "    \n",
    "    #parsing a midi file\n",
    "    midi = converter.parse(file)\n",
    "  \n",
    "    #grouping based on different instruments\n",
    "    s2 = instrument.partitionByInstrument(midi)\n",
    "\n",
    "    #Looping over all the instruments\n",
    "    for part in s2.parts:\n",
    "    \n",
    "        #select elements of only piano\n",
    "        if 'Piano' in str(part): \n",
    "        \n",
    "            notes_to_parse = part.recurse() \n",
    "      \n",
    "            #finding whether a particular element is note or a chord\n",
    "            for element in notes_to_parse:\n",
    "                \n",
    "                #note\n",
    "                if isinstance(element, note.Note):\n",
    "                    notes.append(str(element.pitch))\n",
    "                \n",
    "                #chord\n",
    "                elif isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    return np.array(notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Music File: schubert/schubert_D850_1.mid\n",
      "Loading Music File: schubert/schubert_D850_2.mid\n",
      "Loading Music File: schubert/schubert_D850_3.mid\n",
      "Loading Music File: schubert/schubert_D850_4.mid\n",
      "Loading Music File: schubert/schubert_D935_1.mid\n",
      "Loading Music File: schubert/schubert_D935_2.mid\n",
      "Loading Music File: schubert/schubert_D935_3.mid\n",
      "Loading Music File: schubert/schubert_D935_4.mid\n",
      "Loading Music File: schubert/schub_d760_1.mid\n",
      "Loading Music File: schubert/schub_d760_2.mid\n",
      "Loading Music File: schubert/schub_d760_3.mid\n",
      "Loading Music File: schubert/schub_d760_4.mid\n",
      "Loading Music File: schubert/schub_d960_1.mid\n",
      "Loading Music File: schubert/schub_d960_2.mid\n",
      "Loading Music File: schubert/schub_d960_3.mid\n",
      "Loading Music File: schubert/schub_d960_4.mid\n",
      "Loading Music File: schubert/schuim-1.mid\n",
      "Loading Music File: schubert/schuim-2.mid\n",
      "Loading Music File: schubert/schuim-3.mid\n",
      "Loading Music File: schubert/schuim-4.mid\n",
      "Loading Music File: schubert/schumm-1.mid\n",
      "Loading Music File: schubert/schumm-2.mid\n",
      "Loading Music File: schubert/schumm-3.mid\n",
      "Loading Music File: schubert/schumm-4.mid\n",
      "Loading Music File: schubert/schumm-5.mid\n",
      "Loading Music File: schubert/schumm-6.mid\n",
      "Loading Music File: schubert/schu_143_1.mid\n",
      "Loading Music File: schubert/schu_143_2.mid\n",
      "Loading Music File: schubert/schu_143_3.mid\n"
     ]
    }
   ],
   "source": [
    "#for listing down the file names\n",
    "import os\n",
    "\n",
    "#Array Processing\n",
    "import numpy as np\n",
    "\n",
    "#specify the path\n",
    "path='schubert/'\n",
    "\n",
    "#read all the filenames\n",
    "files=[i for i in os.listdir(path) if i.endswith(\".mid\")]\n",
    "\n",
    "#reading each midi file\n",
    "notes_array = np.array([read_midi(path+i) for i in files])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304\n"
     ]
    }
   ],
   "source": [
    "#converting 2D array into 1D array\n",
    "notes_ = [element for note_ in notes_array for element in note_]\n",
    "\n",
    "#No. of unique notes\n",
    "unique_notes = list(set(notes_))\n",
    "print(len(unique_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([187.,  41.,  26.,  11.,   6.,   9.,  12.,   6.,   3.,   3.]),\n",
       " array([1.0000e+00, 1.4790e+02, 2.9480e+02, 4.4170e+02, 5.8860e+02,\n",
       "        7.3550e+02, 8.8240e+02, 1.0293e+03, 1.1762e+03, 1.3231e+03,\n",
       "        1.4700e+03]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT8AAAEvCAYAAAAzcMYwAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAAsTAAALEwEAmpwYAAARuUlEQVR4nO3dfYxldX3H8fenrGJ9iIA70q2wncWgCTZ1xQnR+BDqI6ABbQxlYxTUdrXVRFsTA5qobWKCD2hr2oKrUrDBFRVRglil1EiaVHRXEZeHlQWXuptldwWrVo114ds/7hm4bGfdmXvPnZnl934lN3PO75xzf9/5zb2fOQ9z7qSqkKTW/M5SFyBJS8Hwk9Qkw09Skww/SU0y/CQ1yfCT1KQVS10AwMqVK2t6enqpy5D0MLN58+YfV9XUXMuWRfhNT0+zadOmpS5D0sNMkrsOtMzDXklNMvwkNcnwk9Qkw09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUpGVxb+8ops/98sT72H7+yybeh6Sl4Z6fpCYZfpKaZPhJapLhJ6lJhp+kJhl+kppk+ElqkuEnqUmGn6QmGX6SmmT4SWqS4SepSQcNvyQXJ9mTZMtQ2+VJbuwe25Pc2LVPJ/nV0LKLJli7JI1sPp/qcgnwD8CnZhuq6k9np5NcAPx0aP07qmptT/VJ0kQcNPyq6vok03MtSxLgTOAFPdclSRM17jm/5wG7q+r2obY1Sb6b5BtJnjfm80vSRIz7YabrgI1D87uA1VV1T5JnAl9M8rSq+tn+GyZZD6wHWL169ZhlSNLCjLznl2QF8CfA5bNtVfXrqrqnm94M3AE8Za7tq2pDVc1U1czU1NSoZUjSSMY57H0RcFtV7ZhtSDKV5LBu+jjgeODO8UqUpP7N509dNgL/CTw1yY4kb+gWncVDD3kBng/c1P3py+eBN1XVvT3WK0m9mM/V3nUHaD9njrYrgCvGL0uSJss7PCQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpMMP0lNMvwkNcnwk9Qkw09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpMMP0lNMvwkNcnwk9Qkw09Skww/SU0y/CQ16aDhl+TiJHuSbBlqe2+SnUlu7B6nDS07L8m2JFuTvHRShUvSOOaz53cJcMoc7R+pqrXd4xqAJCcAZwFP67b5pySH9VWsJPXloOFXVdcD987z+c4APlNVv66qHwLbgJPGqE+SJmKcc35vSXJTd1h8ZNf2JOBHQ+vs6NokaVkZNfwuBJ4MrAV2ARcs9AmSrE+yKcmmvXv3jliGJI1mpPCrqt1VdV9V3Q98nAcPbXcCxw6tekzXNtdzbKiqmaqamZqaGqUMSRrZSOGXZNXQ7CuB2SvBVwFnJTk8yRrgeOBb45UoSf1bcbAVkmwETgZWJtkBvAc4OclaoIDtwBsBqurmJJ8FbgH2AW+uqvsmUrkkjeGg4VdV6+Zo/uRvWf99wPvGKUqSJs07PCQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpMMP0lNMvwkNcnwk9Qkw09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpMMP0lNMvwkNcnwk9Qkw09Skww/SU0y/CQ1yfCT1KSDhl+Si5PsSbJlqO2DSW5LclOSK5Mc0bVPJ/lVkhu7x0UTrF2SRjafPb9LgFP2a7sW+MOq+iPgB8B5Q8vuqKq13eNN/ZQpSf06aPhV1fXAvfu1fa2q9nWz3wSOmUBtkjQxfZzzez3wlaH5NUm+m+QbSZ7Xw/NLUu9WjLNxkncB+4DLuqZdwOqquifJM4EvJnlaVf1sjm3XA+sBVq9ePU4ZkrRgI+/5JTkHeDnw6qoqgKr6dVXd001vBu4AnjLX9lW1oapmqmpmampq1DIkaSQjhV+SU4B3AKdX1S+H2qeSHNZNHwccD9zZR6GS1KeDHvYm2QicDKxMsgN4D4Oru4cD1yYB+GZ3Zff5wN8m+Q1wP/Cmqrp3zieWpCV00PCrqnVzNH/yAOteAVwxblGSNGne4SGpSYafpCYZfpKaZPhJapLhJ6lJhp+kJhl+kppk+ElqkuEnqUmGn6QmGX6SmmT4SWqS4SepSYafpCYZfpKaZPhJapLhJ6lJhp+kJhl+kppk+ElqkuEnqUmGn6QmGX6SmmT4SWqS4SepSYafpCbNK/ySXJxkT5ItQ21HJbk2ye3d1yO79iT5aJJtSW5KcuKkipekUc13z+8S4JT92s4Frquq44HrunmAU4Hju8d64MLxy5Skfs0r/KrqeuDe/ZrPAC7tpi8FXjHU/qka+CZwRJJVPdQqSb0Z55zf0VW1q5u+Gzi6m34S8KOh9XZ0bQ+RZH2STUk27d27d4wyJGnherngUVUF1AK32VBVM1U1MzU11UcZkjRv44Tf7tnD2e7rnq59J3Ds0HrHdG2StGyME35XAWd302cDXxpqf2131fdZwE+HDo8laVlYMZ+VkmwETgZWJtkBvAc4H/hskjcAdwFndqtfA5wGbAN+Cbyu55olaWzzCr+qWneARS+cY90C3jxOUZI0ad7hIalJhp+kJhl+kppk+ElqkuEnqUmGn6QmGX6SmmT4SWqS4SepSYafpCYZfpKaZPhJapLhJ6lJhp+kJhl+kppk+ElqkuEnqUmGn6QmGX6SmmT4SWqS4SepSYafpCYZfpKaZPhJapLhJ6lJK0bdMMlTgcuHmo4D3g0cAfw5sLdrf2dVXTNqP5I0CSOHX1VtBdYCJDkM2AlcCbwO+EhVfaiPAiVpEvo67H0hcEdV3dXT80nSRPUVfmcBG4fm35LkpiQXJzmypz4kqTdjh1+SRwKnA5/rmi4EnszgkHgXcMEBtlufZFOSTXv37p1rFUmamD72/E4FvlNVuwGqandV3VdV9wMfB06aa6Oq2lBVM1U1MzU11UMZkjR/fYTfOoYOeZOsGlr2SmBLD31IUq9GvtoLkOQxwIuBNw41fyDJWqCA7fstk6RlYazwq6pfAE/Yr+01Y1UkSYvAOzwkNcnwk9Qkw09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpMMP0lNMvwkNcnwk9Qkw09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpMMP0lNMvwkNcnwk9SkFeM+QZLtwM+B+4B9VTWT5CjgcmAa2A6cWVU/GbcvSepLX3t+f1xVa6tqpps/F7iuqo4HruvmJWnZmNRh7xnApd30pcArJtSPJI2kj/Ar4GtJNidZ37UdXVW7uum7gaN76EeSejP2OT/guVW1M8kTgWuT3Da8sKoqSe2/UReU6wFWr17dQxmSNH9j7/lV1c7u6x7gSuAkYHeSVQDd1z1zbLehqmaqamZqamrcMiRpQcYKvySPSfK42WngJcAW4Crg7G61s4EvjdOPJPVt3MPeo4Erk8w+16er6l+TfBv4bJI3AHcBZ47ZjyT1aqzwq6o7gafP0X4P8MJxnluSJsk7PCQ1yfCT1CTDT1KTDD9JTTL8JDXJ8JPUJMNPUpP6uLf3YWv63C8vSj/bz3/ZovQj6UHu+UlqkuEnqUmGn6QmGX6SmmT4SWqS4SepSYafpCYZfpKaZPhJapLhJ6lJhp+kJhl+kppk+ElqkuEnqUmGn6QmGX6SmmT4SWrSyOGX5NgkX09yS5Kbk7y1a39vkp1Jbuwep/VXriT1Y5yPsd8HvL2qvpPkccDmJNd2yz5SVR8avzxJmoyRw6+qdgG7uumfJ7kVeFJfhUnSJPXyD4ySTAPPAG4AngO8JclrgU0M9g5/0kc/D1eL8Y+S/CdJ0kONfcEjyWOBK4C3VdXPgAuBJwNrGewZXnCA7dYn2ZRk0969e8ctQ5IWZKzwS/IIBsF3WVV9AaCqdlfVfVV1P/Bx4KS5tq2qDVU1U1UzU1NT45QhSQs2ztXeAJ8Ebq2qDw+1rxpa7ZXAltHLk6TJGOec33OA1wDfT3Jj1/ZOYF2StUAB24E3jtGHJE3EOFd7/wPIHIuuGb0cSVoc3uEhqUmGn6QmGX6SmmT4SWqS4SepSYafpCYZfpKaZPhJapLhJ6lJhp+kJhl+kppk+ElqkuEnqUm9fIy99HCzGP9aAPz3AkvJPT9JTXLPrxH+kyTpodzzk9Qk9/x0yFms83F6eHPPT1KTDD9JTTL8JDXJc37qjefiFs6r8EvHPT9JTTL8JDXJ8JPUJMNPUpMmdsEjySnA3wOHAZ+oqvMn1ZekA3s4XYjq8+LNRPb8khwG/CNwKnACsC7JCZPoS5JGManD3pOAbVV1Z1X9L/AZ4IwJ9SVJCzap8HsS8KOh+R1dmyQtC0v2R85J1gPru9n/SbJ1gU+xEvhxv1WNZLnUAcunluVSByyfWpZLHXAI15L3L/j5/+BACyYVfjuBY4fmj+naHlBVG4ANo3aQZFNVzYy6fV+WSx2wfGpZLnXA8qlludQB1jJrUoe93waOT7ImySOBs4CrJtSXJC3YRPb8qmpfkrcAX2Xwpy4XV9XNk+hLkkYxsXN+VXUNcM2knp8xDpl7tlzqgOVTy3KpA5ZPLculDrAWAFJVS9W3JC0Zb2+T1KRDLvySnJJka5JtSc5dhP6OTfL1JLckuTnJW7v2o5Jcm+T27uuRXXuSfLSr76YkJ/Zcz2FJvpvk6m5+TZIbuv4u7y4wkeTwbn5bt3y65zqOSPL5JLcluTXJs5diTJL8Vfdz2ZJkY5JHLdaYJLk4yZ4kW4baFjwGSc7u1r89ydk91vLB7udzU5IrkxwxtOy8rpatSV461D7W+2uuOoaWvT1JJVnZzU90TA6qqg6ZB4OLJ3cAxwGPBL4HnDDhPlcBJ3bTjwN+wOCWvQ8A53bt5wLv76ZPA74CBHgWcEPP9fw18Gng6m7+s8BZ3fRFwF90038JXNRNnwVc3nMdlwJ/1k0/EjhisceEwR/O/xD43aGxOGexxgR4PnAisGWobUFjABwF3Nl9PbKbPrKnWl4CrOim3z9Uywnde+dwYE33njqsj/fXXHV07ccyuAB6F7ByMcbkoLX2/YSTfADPBr46NH8ecN4i1/Al4MXAVmBV17YK2NpNfwxYN7T+A+v10PcxwHXAC4CruxfNj4de4A+MT/dCe3Y3vaJbLz3V8fgudLJf+6KOCQ/eSXRU9z1eDbx0MccEmN4vcBY0BsA64GND7Q9Zb5xa9lv2SuCybvoh75vZcenr/TVXHcDngacD23kw/CY+Jr/tcagd9i7pbXPdYdIzgBuAo6tqV7fobuDoRajx74B3APd3808A/ruq9s3R1wN1dMt/2q3fhzXAXuCfu0PwTyR5DIs8JlW1E/gQ8F/ALgbf42aWZkxmLXQMFus1/XoGe1mLXkuSM4CdVfW9/RYt6ZgcauG3ZJI8FrgCeFtV/Wx4WQ1+PU30snmSlwN7qmrzJPuZpxUMDm0urKpnAL9gcIj3gEUakyMZfGDGGuD3gccAp0yyz4VYjDGYjyTvAvYBly1B348G3gm8e7H7PphDLfwOetvcJCR5BIPgu6yqvtA1706yqlu+Ctgz4RqfA5yeZDuDT8l5AYPPSzwiyezfaw739UAd3fLHA/f0UAcMfhPvqKobuvnPMwjDxR6TFwE/rKq9VfUb4AsMxmkpxmTWQsdgoq/pJOcALwde3YXxYtfyZAa/nL7XvXaPAb6T5PcWuY7/51ALv0W/bS5JgE8Ct1bVh4cWXQXMXoU6m8G5wNn213ZXsp4F/HToMGhkVXVeVR1TVdMMvu9/r6pXA18HXnWAOmbre1W3fi97IVV1N/CjJE/tml4I3MIijwmDw91nJXl093OarWPRx2TIQsfgq8BLkhzZ7cm+pGsbWwYfKPwO4PSq+uV+NZ7VXf1eAxwPfIsJvL+q6vtV9cSqmu5euzsYXEC8myUYk/2LO6QeDK4Q/YDBVal3LUJ/z2Vw6HITcGP3OI3BuaLrgNuBfwOO6tYPgw9yvQP4PjAzgZpO5sGrvccxeOFuAz4HHN61P6qb39YtP67nGtYCm7px+SKDq3KLPibA3wC3AVuAf2FwBXNRxgTYyOBc428YvKnfMMoYMDgft617vK7HWrYxOHc2+7q9aGj9d3W1bAVO7ev9NVcd+y3fzoMXPCY6Jgd7eIeHpCYdaoe9ktQLw09Skww/SU0y/CQ1yfCT1CTDT1KTDD9JTTL8JDXp/wA7m26QG7spYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#importing library\n",
    "from collections import Counter\n",
    "\n",
    "#computing frequency of each note\n",
    "freq = dict(Counter(notes_))\n",
    "\n",
    "#library for visualiation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#consider only the frequencies\n",
    "no=[count for _,count in freq.items()]\n",
    "\n",
    "#set the figure size\n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#plot\n",
    "plt.hist(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n"
     ]
    }
   ],
   "source": [
    "frequent_notes = [note_ for note_, count in freq.items() if count>=50]\n",
    "print(len(frequent_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_music=[]\n",
    "\n",
    "for notes in notes_array:\n",
    "    temp=[]\n",
    "    for note_ in notes:\n",
    "        if note_ in frequent_notes:\n",
    "            temp.append(note_)            \n",
    "    new_music.append(temp)\n",
    "    \n",
    "new_music = np.array(new_music)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a dictonary to map the unique_notes to integers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note_to_int = list((note,number) for number, note in enumerate(new_music))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preparing the input and output as mentioned in the article\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_timesteps = 32\n",
    "x = []\n",
    "y = []\n",
    "\n",
    "for note_ in new_music:\n",
    "    for i in range(0, len(note_) - no_of_timesteps, 1):\n",
    "        \n",
    "        #preparing input and output sequences\n",
    "        input_ = note_[i:i + no_of_timesteps]\n",
    "        output = note_[i + no_of_timesteps]\n",
    "        \n",
    "        x.append(input_)\n",
    "        y.append(output)\n",
    "        \n",
    "        \n",
    "   \n",
    "    \n",
    "        \n",
    "x=np.array(x)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_patterns = len(x)\n",
    "    \n",
    "    #rehsape the input \n",
    "#    x = np.reshape(x, (n_patterns, no_of_timesteps,1))\n",
    "    \n",
    "    #normalize the input\n",
    " #   x = x / float(n_vocab)\n",
    "    \n",
    "  #  y = np_utils.to_sparse_categorical(y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## asign a unique interger to every note \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_x = list(set(x.ravel()))\n",
    "x_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare the integer sequences for input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_seq=[]\n",
    "for i in x:\n",
    "    temp=[]\n",
    "    for j in i:\n",
    "        #assigning unique integer to every note\n",
    "        temp.append(x_note_to_int[j])\n",
    "    x_seq.append(temp)\n",
    "    \n",
    "x_seq = np.array(x_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do the same for the output as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_y = list(set(y))\n",
    "y_note_to_int = dict((note_, number) for number, note_ in enumerate(unique_y)) \n",
    "y_seq=np.array([y_note_to_int[i] for i in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(x_seq, y_seq, test_size = 0.2,random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm():\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(128,return_sequences=True))\n",
    "  model.add(LSTM(128))\n",
    "  model.add(Dense(256))\n",
    "  model.add(Activation('relu'))\n",
    "  model.add(Dense(n_vocab))\n",
    "  model.add(Activation('softmax'))\n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy\n",
    "from music21 import converter, instrument, note, chord\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization as BatchNorm\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 32, 100)           16700     \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 32, 64)            19264     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 16, 64)            12352     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 64)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 8, 64)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 8, 128)            24704     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 8, 128)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 4, 256)            98560     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 256)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 2, 64)             49216     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 2, 64)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 167)               42919     \n",
      "=================================================================\n",
      "Total params: 346,147\n",
      "Trainable params: 346,147\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from keras.callbacks import *\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import MaxPool1D\n",
    "from tensorflow.keras.layers import GlobalMaxPool1D\n",
    "#from tensorflow.keras.models import Embedding\n",
    "K.clear_session()\n",
    "model = Sequential()\n",
    "    \n",
    "#embedding layer\n",
    "model.add(Embedding(len(unique_x), 100, input_length=32,trainable=True)) \n",
    "\n",
    "model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "    \n",
    "model.add(Conv1D(128,3,activation='relu',dilation_rate=2,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(256,3,activation='relu',dilation_rate=4,padding='causal'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "model.add(Conv1D(64,3, padding='causal',activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPool1D(2))\n",
    "\n",
    "\n",
    "#model.add(Conv1D(256,5,activation='relu'))    \n",
    "model.add(GlobalMaxPool1D())\n",
    "    \n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(len(unique_y), activation='softmax'))\n",
    "    \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    " mc=ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', save_best_only=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51530 samples, validate on 12883 samples\n",
      "Epoch 1/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 4.5972\n",
      "Epoch 00001: val_loss improved from inf to 4.54334, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 103s 2ms/sample - loss: 4.5971 - val_loss: 4.5433\n",
      "Epoch 2/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 4.3735\n",
      "Epoch 00002: val_loss improved from 4.54334 to 4.33599, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 32s 628us/sample - loss: 4.3731 - val_loss: 4.3360\n",
      "Epoch 3/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 4.1516- ETA: 0s - l\n",
      "Epoch 00003: val_loss improved from 4.33599 to 4.13970, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 29s 567us/sample - loss: 4.1514 - val_loss: 4.1397\n",
      "Epoch 4/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.9783\n",
      "Epoch 00004: val_loss improved from 4.13970 to 3.98419, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 31s 599us/sample - loss: 3.9784 - val_loss: 3.9842\n",
      "Epoch 5/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.8732\n",
      "Epoch 00005: val_loss improved from 3.98419 to 3.92583, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 30s 576us/sample - loss: 3.8731 - val_loss: 3.9258\n",
      "Epoch 6/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.8027\n",
      "Epoch 00006: val_loss improved from 3.92583 to 3.83132, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 31s 608us/sample - loss: 3.8029 - val_loss: 3.8313\n",
      "Epoch 7/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.7302\n",
      "Epoch 00007: val_loss improved from 3.83132 to 3.76151, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 30s 583us/sample - loss: 3.7302 - val_loss: 3.7615\n",
      "Epoch 8/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.6642\n",
      "Epoch 00008: val_loss did not improve from 3.76151\n",
      "51530/51530 [==============================] - 29s 557us/sample - loss: 3.6640 - val_loss: 3.7677\n",
      "Epoch 9/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.6048\n",
      "Epoch 00009: val_loss improved from 3.76151 to 3.65989, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 32s 616us/sample - loss: 3.6047 - val_loss: 3.6599\n",
      "Epoch 10/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.5612\n",
      "Epoch 00010: val_loss improved from 3.65989 to 3.61297, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 30s 576us/sample - loss: 3.5614 - val_loss: 3.6130\n",
      "Epoch 11/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.5187\n",
      "Epoch 00011: val_loss did not improve from 3.61297\n",
      "51530/51530 [==============================] - 29s 566us/sample - loss: 3.5187 - val_loss: 3.6170\n",
      "Epoch 12/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.4833\n",
      "Epoch 00012: val_loss improved from 3.61297 to 3.56654, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 30s 584us/sample - loss: 3.4835 - val_loss: 3.5665\n",
      "Epoch 13/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.4460\n",
      "Epoch 00013: val_loss improved from 3.56654 to 3.52728, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 31s 607us/sample - loss: 3.4462 - val_loss: 3.5273\n",
      "Epoch 14/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.4236\n",
      "Epoch 00014: val_loss improved from 3.52728 to 3.52358, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 37s 719us/sample - loss: 3.4238 - val_loss: 3.5236\n",
      "Epoch 15/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.3869\n",
      "Epoch 00015: val_loss improved from 3.52358 to 3.46055, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 32s 627us/sample - loss: 3.3870 - val_loss: 3.4606\n",
      "Epoch 16/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.3619\n",
      "Epoch 00016: val_loss did not improve from 3.46055\n",
      "51530/51530 [==============================] - 27s 523us/sample - loss: 3.3617 - val_loss: 3.4654\n",
      "Epoch 17/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.3325- ETA: 0s - loss: 3.\n",
      "Epoch 00017: val_loss improved from 3.46055 to 3.42548, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 29s 567us/sample - loss: 3.3325 - val_loss: 3.4255\n",
      "Epoch 18/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.3167\n",
      "Epoch 00018: val_loss improved from 3.42548 to 3.38714, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 29s 563us/sample - loss: 3.3167 - val_loss: 3.3871\n",
      "Epoch 19/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.2870\n",
      "Epoch 00019: val_loss did not improve from 3.38714\n",
      "51530/51530 [==============================] - 29s 558us/sample - loss: 3.2871 - val_loss: 3.3965\n",
      "Epoch 20/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.2702\n",
      "Epoch 00020: val_loss did not improve from 3.38714\n",
      "51530/51530 [==============================] - 29s 569us/sample - loss: 3.2699 - val_loss: 3.3875\n",
      "Epoch 21/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.2467\n",
      "Epoch 00021: val_loss improved from 3.38714 to 3.37943, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 30s 574us/sample - loss: 3.2468 - val_loss: 3.3794\n",
      "Epoch 22/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.2265\n",
      "Epoch 00022: val_loss improved from 3.37943 to 3.33193, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 27s 526us/sample - loss: 3.2270 - val_loss: 3.3319\n",
      "Epoch 23/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.2160\n",
      "Epoch 00023: val_loss improved from 3.33193 to 3.32603, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 30s 589us/sample - loss: 3.2159 - val_loss: 3.3260\n",
      "Epoch 24/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.1931\n",
      "Epoch 00024: val_loss improved from 3.32603 to 3.31140, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 30s 580us/sample - loss: 3.1931 - val_loss: 3.3114\n",
      "Epoch 25/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.1786\n",
      "Epoch 00025: val_loss improved from 3.31140 to 3.27919, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 29s 561us/sample - loss: 3.1787 - val_loss: 3.2792\n",
      "Epoch 26/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.1623\n",
      "Epoch 00026: val_loss improved from 3.27919 to 3.27357, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 29s 560us/sample - loss: 3.1623 - val_loss: 3.2736\n",
      "Epoch 27/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.1451\n",
      "Epoch 00027: val_loss did not improve from 3.27357\n",
      "51530/51530 [==============================] - 28s 546us/sample - loss: 3.1449 - val_loss: 3.2809\n",
      "Epoch 28/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.1351\n",
      "Epoch 00028: val_loss improved from 3.27357 to 3.25337, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 29s 560us/sample - loss: 3.1349 - val_loss: 3.2534\n",
      "Epoch 29/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.1212\n",
      "Epoch 00029: val_loss improved from 3.25337 to 3.24792, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 36s 695us/sample - loss: 3.1212 - val_loss: 3.2479\n",
      "Epoch 30/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.1008\n",
      "Epoch 00030: val_loss improved from 3.24792 to 3.22820, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 29s 564us/sample - loss: 3.1010 - val_loss: 3.2282\n",
      "Epoch 31/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.0914\n",
      "Epoch 00031: val_loss improved from 3.22820 to 3.22499, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 27s 533us/sample - loss: 3.0915 - val_loss: 3.2250\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.0818\n",
      "Epoch 00032: val_loss improved from 3.22499 to 3.20559, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 29s 570us/sample - loss: 3.0820 - val_loss: 3.2056\n",
      "Epoch 33/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.0693\n",
      "Epoch 00033: val_loss improved from 3.20559 to 3.18354, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 30s 580us/sample - loss: 3.0690 - val_loss: 3.1835\n",
      "Epoch 34/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.0590- ETA: 0s - loss - ETA: 0s - loss: \n",
      "Epoch 00034: val_loss did not improve from 3.18354\n",
      "51530/51530 [==============================] - 26s 513us/sample - loss: 3.0592 - val_loss: 3.1977\n",
      "Epoch 35/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.0479\n",
      "Epoch 00035: val_loss did not improve from 3.18354\n",
      "51530/51530 [==============================] - 26s 510us/sample - loss: 3.0479 - val_loss: 3.1904\n",
      "Epoch 36/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.0321\n",
      "Epoch 00036: val_loss did not improve from 3.18354\n",
      "51530/51530 [==============================] - 27s 523us/sample - loss: 3.0323 - val_loss: 3.1892\n",
      "Epoch 37/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.0210\n",
      "Epoch 00037: val_loss improved from 3.18354 to 3.15086, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 27s 525us/sample - loss: 3.0210 - val_loss: 3.1509\n",
      "Epoch 38/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.0130\n",
      "Epoch 00038: val_loss improved from 3.15086 to 3.15010, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 27s 533us/sample - loss: 3.0129 - val_loss: 3.1501\n",
      "Epoch 39/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.0032\n",
      "Epoch 00039: val_loss improved from 3.15010 to 3.14718, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 27s 531us/sample - loss: 3.0032 - val_loss: 3.1472\n",
      "Epoch 40/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 3.0040\n",
      "Epoch 00040: val_loss improved from 3.14718 to 3.12651, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 26s 509us/sample - loss: 3.0039 - val_loss: 3.1265\n",
      "Epoch 41/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 2.9804\n",
      "Epoch 00041: val_loss did not improve from 3.12651\n",
      "51530/51530 [==============================] - 25s 488us/sample - loss: 2.9804 - val_loss: 3.1623\n",
      "Epoch 42/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 2.9848- ETA:\n",
      "Epoch 00042: val_loss did not improve from 3.12651\n",
      "51530/51530 [==============================] - 25s 487us/sample - loss: 2.9843 - val_loss: 3.1333\n",
      "Epoch 43/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 2.9746\n",
      "Epoch 00043: val_loss improved from 3.12651 to 3.11829, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 26s 498us/sample - loss: 2.9747 - val_loss: 3.1183\n",
      "Epoch 44/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 2.9653\n",
      "Epoch 00044: val_loss did not improve from 3.11829\n",
      "51530/51530 [==============================] - 25s 489us/sample - loss: 2.9654 - val_loss: 3.1292\n",
      "Epoch 45/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 2.9583\n",
      "Epoch 00045: val_loss improved from 3.11829 to 3.11365, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 26s 511us/sample - loss: 2.9584 - val_loss: 3.1137\n",
      "Epoch 46/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 2.9488\n",
      "Epoch 00046: val_loss improved from 3.11365 to 3.10931, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 26s 508us/sample - loss: 2.9485 - val_loss: 3.1093\n",
      "Epoch 47/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 2.9471\n",
      "Epoch 00047: val_loss did not improve from 3.10931\n",
      "51530/51530 [==============================] - 25s 490us/sample - loss: 2.9469 - val_loss: 3.1351\n",
      "Epoch 48/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 2.9395- \n",
      "Epoch 00048: val_loss improved from 3.10931 to 3.09863, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 26s 505us/sample - loss: 2.9392 - val_loss: 3.0986\n",
      "Epoch 49/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 2.9234\n",
      "Epoch 00049: val_loss improved from 3.09863 to 3.07869, saving model to best_model.h5\n",
      "51530/51530 [==============================] - 26s 509us/sample - loss: 2.9237 - val_loss: 3.0787\n",
      "Epoch 50/50\n",
      "51456/51530 [============================>.] - ETA: 0s - loss: 2.9213-\n",
      "Epoch 00050: val_loss did not improve from 3.07869\n",
      "51530/51530 [==============================] - 25s 483us/sample - loss: 2.9212 - val_loss: 3.0900\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(np.array(x_tr),np.array(y_tr),batch_size=128,epochs=50, validation_data=(np.array(x_val),np.array(y_val)),verbose=1, callbacks=[mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading best model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
